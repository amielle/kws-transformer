{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install pytorch-lightning --upgrade\n%pip install torchmetrics --upgrade\n!pip install einops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-28T15:02:55.348856Z","iopub.execute_input":"2022-05-28T15:02:55.349205Z","iopub.status.idle":"2022-05-28T15:03:25.148580Z","shell.execute_reply.started":"2022-05-28T15:02:55.349129Z","shell.execute_reply":"2022-05-28T15:03:25.147567Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio, torchvision\nimport os\nimport matplotlib.pyplot as plt \nimport librosa\nimport argparse\nimport numpy as np\nimport wandb\nfrom pytorch_lightning import LightningModule, Trainer, LightningDataModule, Callback\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom torchmetrics.functional import accuracy\nfrom torchvision.transforms import ToTensor\nfrom torchaudio.datasets import SPEECHCOMMANDS\nfrom torchaudio.datasets.speechcommands import load_speechcommands_item","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:03:25.150711Z","iopub.execute_input":"2022-05-28T15:03:25.151113Z","iopub.status.idle":"2022-05-28T15:03:35.673405Z","shell.execute_reply.started":"2022-05-28T15:03:25.151073Z","shell.execute_reply":"2022-05-28T15:03:35.672581Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class SilenceDataset(SPEECHCOMMANDS):\n    def __init__(self, root):\n        super(SilenceDataset, self).__init__(root, subset='training')\n        self.len = len(self._walker) // 35\n        path = os.path.join(self._path, torchaudio.datasets.speechcommands.EXCEPT_FOLDER)\n        self.paths = [os.path.join(path, p) for p in os.listdir(path) if p.endswith('.wav')]\n\n    def __getitem__(self, index):\n        index = np.random.randint(0, len(self.paths))\n        filepath = self.paths[index]\n        waveform, sample_rate = torchaudio.load(filepath)\n        return waveform, sample_rate, \"silence\", 0, 0\n\n    def __len__(self):\n        return self.len\n\nclass UnknownDataset(SPEECHCOMMANDS):\n    def __init__(self, root):\n        super(UnknownDataset, self).__init__(root, subset='training')\n        self.len = len(self._walker) // 35\n\n    def __getitem__(self, index):\n        index = np.random.randint(0, len(self._walker))\n        fileid = self._walker[index]\n        waveform, sample_rate, _, speaker_id, utterance_number = load_speechcommands_item(fileid, self._path)\n        return waveform, sample_rate, \"unknown\", speaker_id, utterance_number\n\n    def __len__(self):\n        return self.len\n\n\nclass KWSDataModule(LightningDataModule):\n    def __init__(self, path, batch_size=32, num_workers=0, n_fft=512, \n                 n_mels=128, win_length=None, hop_length=256, patch_num=4, class_dict={}, \n                 **kwargs):\n        super().__init__(**kwargs)\n        self.path = path\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.n_fft = n_fft\n        self.n_mels = n_mels\n        self.win_length = win_length\n        self.hop_length = hop_length\n        self.class_dict = class_dict\n        self.patch_num = patch_num\n\n    def prepare_data(self):\n        self.train_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n                                                                download=True,\n                                                                subset='training')\n\n        silence_dataset = SilenceDataset(self.path)\n        unknown_dataset = UnknownDataset(self.path)\n        self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n                                                                \n        self.val_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n                                                              download=True,\n                                                              subset='validation')\n        self.test_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n                                                               download=True,\n                                                               subset='testing')                                                    \n        _, sample_rate, _, _, _ = self.train_dataset[0]\n        self.sample_rate = sample_rate\n        self.transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n                                                              n_fft=self.n_fft,\n                                                              win_length=self.win_length,\n                                                              hop_length=self.hop_length,\n                                                              n_mels=self.n_mels,\n                                                              power=2.0)\n\n    def setup(self, stage=None):\n        self.prepare_data()\n\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=True,\n            pin_memory=True,\n            collate_fn=self.collate_fn\n        )\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=True,\n            pin_memory=True,\n            collate_fn=self.collate_fn\n        )\n    \n    def test_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=True,\n            pin_memory=True,\n            collate_fn=self.collate_fn\n        )\n\n    def collate_fn(self, batch):\n        mels = []\n        labels = []\n        wavs = []\n        for sample in batch:\n            waveform, sample_rate, label, speaker_id, utterance_number = sample\n            # ensure that all waveforms are 1sec in length; if not pad with zeros\n            if waveform.shape[-1] < sample_rate:\n                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n            elif waveform.shape[-1] > sample_rate:\n                waveform = waveform[:,:sample_rate]\n\n            # mel from power to db\n            mels.append(ToTensor()(librosa.power_to_db(self.transform(waveform).squeeze().numpy(), ref=np.max)))\n            labels.append(torch.tensor(self.class_dict[label]))\n            wavs.append(waveform)\n\n        mels = torch.stack(mels)\n        mels = rearrange(mels, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n        labels = torch.stack(labels)\n        wavs = torch.stack(wavs)\n   \n        return mels, labels, wavs","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:03:35.674889Z","iopub.execute_input":"2022-05-28T15:03:35.675532Z","iopub.status.idle":"2022-05-28T15:03:35.704006Z","shell.execute_reply.started":"2022-05-28T15:03:35.675494Z","shell.execute_reply":"2022-05-28T15:03:35.703053Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\n\nfrom argparse import ArgumentParser\nfrom pytorch_lightning import LightningModule, Trainer, LightningDataModule\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchmetrics.functional import accuracy\nfrom einops import rearrange\nfrom torch import nn\nfrom torchvision.datasets.cifar import CIFAR10\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n\n        return x\n\n\nclass Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n    \"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n      \n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, hidden_features)\n        self.act = act_layer()\n        self.fc3 = nn.Linear(hidden_features, out_features)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        x = self.act(x)\n        x = self.fc3(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n   \n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n                                     act_layer, norm_layer) for _ in range(num_blocks)])\n\n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n\ndef init_weights_vit_timm(module: nn.Module):\n    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n    if isinstance(module, nn.Linear):\n        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()\n\n\nclass KWSTransformer(LightningModule):\n    def __init__(self, num_classes=37, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n                 head=4, patch_dim=192, seqlen=16, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n\n        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n        self.loss = torch.nn.CrossEntropyLoss()\n        \n        self.reset_parameters()\n\n\n    def reset_parameters(self):\n        init_weights_vit_timm(self)\n    \n\n    def forward(self, x):\n        # Linear projection\n        x = self.embed(x)\n            \n        # Encoder\n        x = self.encoder(x)\n        x = x.flatten(start_dim=1)\n\n        # Classification head\n        x = self.fc(x)\n        return x\n    \n    def configure_optimizers(self):\n        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n        # this decays the learning rate to 0 after max_epochs using cosine annealing\n        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        x, y, _ = batch\n        y_hat = self(x)\n        loss = self.loss(y_hat, y)\n        return {'loss': loss}\n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        self.log(\"train_loss\", avg_loss, on_epoch=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y, _ = batch\n        y_hat = self(x)\n        loss = self.loss(y_hat, y)\n        acc = accuracy(y_hat, y) * 100.\n        return {\"preds\": y_hat, 'test_loss': loss, 'test_acc': acc}\n\n    def test_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n        self.log(\"test_acc\", avg_acc, on_epoch=True, prog_bar=True)\n\n    def validation_step(self, batch, batch_idx):\n        return self.test_step(batch, batch_idx)\n\n    def validation_epoch_end(self, outputs):\n        return self.test_epoch_end(outputs)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:03:35.706568Z","iopub.execute_input":"2022-05-28T15:03:35.707151Z","iopub.status.idle":"2022-05-28T15:03:35.744911Z","shell.execute_reply.started":"2022-05-28T15:03:35.707110Z","shell.execute_reply":"2022-05-28T15:03:35.744028Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class WandbCallback(Callback):\n\n    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n        # log 10 sample audio predictions from the first batch\n        if batch_idx == 0:\n            n = 10\n            mels, labels, wavs = batch\n            preds = outputs[\"preds\"]\n            preds = torch.argmax(preds, dim=1)\n\n            labels = labels.cpu().numpy()\n            preds = preds.cpu().numpy()\n            \n            wavs = torch.squeeze(wavs, dim=1)\n            wavs = [ (wav.cpu().numpy()*32768.0).astype(\"int16\") for wav in wavs]\n            \n            sample_rate = pl_module.hparams.sample_rate\n            idx_to_class = pl_module.hparams.idx_to_class\n            \n            # log audio samples and predictions as a W&B Table\n            columns = ['audio', 'mel', 'ground truth', 'prediction']\n            data = [[wandb.Audio(wav, sample_rate=sample_rate), wandb.Image(mel), idx_to_class[label], idx_to_class[pred]] for wav, mel, label, pred in list(\n                zip(wavs[:n], mels[:n], labels[:n], preds[:n]))]\n            wandb_logger.log_table(\n                key='KWS using Transformer andn PyTorch Lightning',\n                columns=columns,\n                data=data)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:03:35.746724Z","iopub.execute_input":"2022-05-28T15:03:35.747223Z","iopub.status.idle":"2022-05-28T15:03:35.758251Z","shell.execute_reply.started":"2022-05-28T15:03:35.747182Z","shell.execute_reply":"2022-05-28T15:03:35.757452Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_args():\n    parser = argparse.ArgumentParser()\n    # model training hyperparameters\n    parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--max-epochs', type=int, default=70, metavar='N',\n                        help='number of epochs to train (default: 30)')\n    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n                        help='learning rate (default: 0.001)')\n\n    # where dataset will be stored\n    parser.add_argument(\"--path\", type=str, default=\"data/speech_commands/\")\n\n    # 35 keywords + silence + unknown\n    parser.add_argument(\"--num-classes\", type=int, default=37)\n   \n    # mel spectrogram parameters\n    parser.add_argument(\"--n-fft\", type=int, default=1024)\n    parser.add_argument(\"--n-mels\", type=int, default=128)\n    parser.add_argument(\"--win-length\", type=int, default=None)\n    parser.add_argument(\"--hop-length\", type=int, default=512)\n\n    # 16-bit fp model to reduce the size\n    parser.add_argument(\"--precision\", default=16)\n    parser.add_argument(\"--accelerator\", default='gpu')\n    parser.add_argument(\"--devices\", default=1)\n    parser.add_argument(\"--num-workers\", type=int, default=2)\n\n    # transformer arguments\n    parser.add_argument('--depth', type=int, default=32, help='depth')\n    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n    parser.add_argument('--num_heads', type=int, default=16, help='num_heads')\n\n    parser.add_argument('--patch_num', type=int, default=4, help='patch_num')\n    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n    \n    parser.add_argument(\"--no-wandb\", default=True)\n\n    args = parser.parse_args(\"\")\n    return args\n\ndef plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n    waveform = waveform.numpy()\n\n    num_channels, num_frames = waveform.shape\n    time_axis = torch.arange(0, num_frames) / sample_rate\n\n    figure, axes = plt.subplots(num_channels, 1)\n    if num_channels == 1:\n        axes = [axes]\n    for c in range(num_channels):\n        axes[c].plot(time_axis, waveform[c], linewidth=1)\n        axes[c].grid(True)\n        if num_channels > 1:\n            axes[c].set_ylabel(f'Channel {c+1}')\n        if xlim:\n            axes[c].set_xlim(xlim)\n        if ylim:\n            axes[c].set_ylim(ylim)\n    figure.suptitle(title)\n    plt.show(block=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:42:31.397322Z","iopub.execute_input":"2022-05-28T15:42:31.397701Z","iopub.status.idle":"2022-05-28T15:42:31.413626Z","shell.execute_reply.started":"2022-05-28T15:42:31.397666Z","shell.execute_reply":"2022-05-28T15:42:31.412818Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"args = get_args()\nCLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n           'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n           'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n           'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n\n# make a dictionary from CLASSES to integers\nCLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\nif not os.path.exists(args.path):\n        os.makedirs(args.path, exist_ok=True)\n\ndatamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n                               path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n                               win_length=args.win_length, hop_length=args.hop_length,\n                               patch_num=args.patch_num,\n                               class_dict=CLASS_TO_IDX)\ndatamodule.setup()\n\n\n# data = iter(datamodule.train_dataloader()).next()\n# patch_dim = data[0].shape[-1]\n# print(data[0].shape[-1])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:42:31.947204Z","iopub.execute_input":"2022-05-28T15:42:31.947715Z","iopub.status.idle":"2022-05-28T15:42:35.014402Z","shell.execute_reply.started":"2022-05-28T15:42:31.947681Z","shell.execute_reply":"2022-05-28T15:42:35.013519Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n\nargs = get_args()\n#     CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n#                'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n#                'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n#                'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n    \n#     # make a dictionary from CLASSES to integers\n#     CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n\n#     if not os.path.exists(args.path):\n#         os.makedirs(args.path, exist_ok=True)\n\n#     datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n#                                path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n#                                win_length=args.win_length, hop_length=args.hop_length,\n#                                patch_num=\n#                                class_dict=CLASS_TO_IDX)\n#     datamodule.setup()\n\n    \ndata = iter(datamodule.train_dataloader()).next()\npatch_dim = data[0].shape[-1]\nseqlen = data[0].shape[-2]\nprint(\"Embed dim:\", args.embed_dim)\nprint(\"Sequence length:\", seqlen)\n\n\nmodel = KWSTransformer(num_classes=37, lr=args.lr, epochs=args.max_epochs, \n                       depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n                       patch_dim=patch_dim, seqlen=seqlen,)\n\n\n# wandb is a great way to debug and visualize this model\n\nmodel_checkpoint = ModelCheckpoint(\n    dirpath=os.path.join(args.path, \"checkpoints\"),\n    filename=\"transformer-kws-best-acc\",\n    save_top_k=1,\n    verbose=True,\n    monitor='test_acc',\n    mode='max',\n)\nidx_to_class = {v: k for k, v in CLASS_TO_IDX.items()}\n\nif not args.no_wandb:\n    import time\n    wandb_logger = WandbLogger(project=f\"kws-{time.time()}\")\n    callbacks = [model_checkpoint, WandbCallback()]\nelse:\n    wandb_logger = None\n    callbacks = [model_checkpoint]\n\ntrainer = Trainer(accelerator=args.accelerator,\n                  devices=args.devices,\n                  precision=args.precision,\n                  max_epochs=args.max_epochs,\n                  logger=wandb_logger if not args.no_wandb else None,\n                  callbacks=callbacks)\nmodel.hparams.sample_rate = datamodule.sample_rate\nmodel.hparams.idx_to_class = idx_to_class\ntrainer.fit(model, datamodule=datamodule)\ntrainer.test(model, datamodule=datamodule)\n\nif not args.no_wandb: wandb.finish()\n\nscript = model.to_torchscript()\nmodel_path = f\"{os.getcwd()}/transformer-model4-checkpoint.pt\"\ntorch.jit.save(script, model_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:42:35.016985Z","iopub.execute_input":"2022-05-28T15:42:35.017527Z","iopub.status.idle":"2022-05-28T23:19:46.178269Z","shell.execute_reply.started":"2022-05-28T15:42:35.017488Z","shell.execute_reply":"2022-05-28T23:19:46.177333Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r\"transformer-model4-checkpoint.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T23:19:46.182008Z","iopub.execute_input":"2022-05-28T23:19:46.182733Z","iopub.status.idle":"2022-05-28T23:19:46.191182Z","shell.execute_reply.started":"2022-05-28T23:19:46.182699Z","shell.execute_reply":"2022-05-28T23:19:46.190311Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}